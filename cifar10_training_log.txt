Using device cuda
Files already downloaded and verified
Training CIFAR-10 without pruning...
Epoch: 1, Loss: 2.4655, Training Accuracy: 25.47%, Validation Accuracy: 37.59%
New best model found and saved with validation accuracy: 37.59%
Epoch: 2, Loss: 1.5518, Training Accuracy: 43.26%, Validation Accuracy: 43.55%
New best model found and saved with validation accuracy: 43.55%
Epoch: 3, Loss: 1.3717, Training Accuracy: 50.63%, Validation Accuracy: 50.44%
New best model found and saved with validation accuracy: 50.44%
Epoch: 4, Loss: 1.2239, Training Accuracy: 56.28%, Validation Accuracy: 58.69%
New best model found and saved with validation accuracy: 58.69%
Epoch: 5, Loss: 1.1125, Training Accuracy: 60.72%, Validation Accuracy: 55.16%
Epoch: 6, Loss: 1.0683, Training Accuracy: 62.14%, Validation Accuracy: 55.58%
Epoch: 7, Loss: 0.9895, Training Accuracy: 65.34%, Validation Accuracy: 67.49%
New best model found and saved with validation accuracy: 67.49%
Epoch: 8, Loss: 1.0633, Training Accuracy: 62.57%, Validation Accuracy: 59.57%
Epoch: 9, Loss: 0.8882, Training Accuracy: 68.73%, Validation Accuracy: 69.80%
New best model found and saved with validation accuracy: 69.80%
Epoch: 10, Loss: 0.7519, Training Accuracy: 73.40%, Validation Accuracy: 69.74%
Pruning CIFAR-10 with 50.0%...
Pruning with threshold 0.0563, Pruned 11727456.0/23454912 weights (50.00% pruned)
Retraining CIFAR-10 after pruning...
Epoch: 1, Loss: 1.0356, Training Accuracy: 65.50%, Validation Accuracy: 61.71%
New best model found and saved with validation accuracy: 61.71%
Epoch: 2, Loss: 0.9315, Training Accuracy: 68.49%, Validation Accuracy: 63.37%
New best model found and saved with validation accuracy: 63.37%
Epoch: 3, Loss: 0.7561, Training Accuracy: 74.03%, Validation Accuracy: 72.66%
New best model found and saved with validation accuracy: 72.66%
Epoch: 4, Loss: 0.7023, Training Accuracy: 76.18%, Validation Accuracy: 73.46%
New best model found and saved with validation accuracy: 73.46%
Epoch: 5, Loss: 0.7233, Training Accuracy: 75.18%, Validation Accuracy: 71.57%
Epoch: 6, Loss: 0.5879, Training Accuracy: 79.69%, Validation Accuracy: 74.73%
New best model found and saved with validation accuracy: 74.73%
Epoch: 7, Loss: 0.4940, Training Accuracy: 83.17%, Validation Accuracy: 74.61%
Epoch: 8, Loss: 0.4252, Training Accuracy: 85.26%, Validation Accuracy: 73.95%
Epoch: 9, Loss: 0.2234, Training Accuracy: 92.57%, Validation Accuracy: 77.84%
New best model found and saved with validation accuracy: 77.84%
Epoch: 10, Loss: 0.1488, Training Accuracy: 95.04%, Validation Accuracy: 77.75%
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 70.0%...
Pruning with threshold 0.2565, Pruned 16418438.0/23454912 weights (70.00% pruned)
Retraining CIFAR-10 after pruning...
Epoch: 1, Loss: 0.4857, Training Accuracy: 84.28%, Validation Accuracy: 60.79%
New best model found and saved with validation accuracy: 60.79%
Epoch: 2, Loss: 0.4518, Training Accuracy: 84.73%, Validation Accuracy: 76.18%
New best model found and saved with validation accuracy: 76.18%
Epoch: 3, Loss: 0.2465, Training Accuracy: 91.42%, Validation Accuracy: 75.63%
Epoch: 4, Loss: 0.3991, Training Accuracy: 87.02%, Validation Accuracy: 75.17%
Epoch: 5, Loss: 0.2218, Training Accuracy: 92.28%, Validation Accuracy: 75.81%
Epoch: 6, Loss: 0.1901, Training Accuracy: 93.42%, Validation Accuracy: 75.02%
Epoch: 7, Loss: 0.1729, Training Accuracy: 94.05%, Validation Accuracy: 74.77%
Epoch: 8, Loss: 0.2486, Training Accuracy: 91.83%, Validation Accuracy: 74.39%
Epoch: 9, Loss: 0.1350, Training Accuracy: 95.59%, Validation Accuracy: 76.64%
New best model found and saved with validation accuracy: 76.64%
Epoch: 10, Loss: 0.0675, Training Accuracy: 97.79%, Validation Accuracy: 76.97%
New best model found and saved with validation accuracy: 76.97%
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0%...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Pruned 0 weights out of 0 weights total
Retraining CIFAR-10 after pruning...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Pruned 0 weights out of 0 weights total
Retraining CIFAR-10 after pruning...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Pruned 0 weights out of 0 weights total
Retraining CIFAR-10 after pruning...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
outputs
tensor([[-4.0897e+00, -8.4356e+00, -3.3774e+00,  8.7212e+00, -3.1788e+00,
         -2.1894e+00, -3.0804e+00, -3.3237e+00, -7.2739e+00, -5.6147e+00],
        [-1.3544e-01,  3.1236e+00, -6.5582e+00, -4.9181e+00, -8.4531e+00,
         -7.3731e+00, -8.9349e+00, -6.9435e+00,  3.7789e+00,  3.1698e-01],
        [ 6.5395e-01, -2.4323e+00, -5.4203e+00, -5.8111e+00, -6.0071e+00,
         -6.9166e+00, -9.0718e+00, -5.9765e+00,  4.4853e+00,  9.6967e-01],
        [ 8.8158e+00, -2.8122e+00, -4.9630e+00, -8.2775e+00, -8.0612e+00,
         -1.0976e+01, -8.4792e+00, -7.4955e+00, -1.9544e-01, -1.9421e+00],
        [-1.0217e+01, -1.0036e+01,  1.8391e+00, -8.8657e-01,  1.0709e+01,
         -9.4129e+00,  1.9790e+00, -1.0395e+01, -9.1686e+00, -1.0626e+01],
        [-8.1268e+00, -4.6585e+00, -4.7824e+00,  1.3708e+00, -3.2282e+00,
          9.1067e-01,  5.1032e+00, -5.0818e+00, -5.2753e+00, -5.5634e+00],
        [-7.5698e-01,  3.7777e+00, -3.6070e+00,  1.6203e+00, -8.4380e+00,
         -2.3425e+00, -6.4160e+00, -1.2906e+00, -8.3086e+00,  9.7409e-01],
        [-8.2265e+00, -7.9486e+00,  4.9222e+00, -3.3212e+00, -1.4888e+00,
         -2.1874e+00,  4.0409e+00, -2.5499e+00, -8.6875e+00, -4.4524e+00],
        [-7.7471e+00, -9.0343e+00, -7.2408e+00,  1.0404e+01, -1.9535e+00,
          5.3611e-01, -9.8068e+00, -3.2079e+00, -1.6148e+01, -9.0191e+00],
        [-6.5220e+00,  3.3675e+00, -8.7540e+00, -7.6591e+00, -1.0479e+01,
         -8.0220e+00, -4.0553e+00, -1.0262e+01, -8.4258e-01,  6.8987e+00],
        [ 5.0642e+00, -8.1276e+00, -1.2530e+00, -1.2252e+00, -7.9625e-01,
         -3.1416e+00, -6.7135e+00, -2.8595e+00, -4.8430e+00, -7.3935e+00],
        [-2.3608e+01, -1.7964e+01, -4.3548e+01, -4.0088e+01, -3.0085e+01,
         -3.3201e+01, -3.3570e+01, -3.1336e+01, -2.4398e+01, -2.6063e+00],
        [-1.0449e+01, -2.8353e+00, -4.3925e+00,  2.5497e+00,  4.8181e+00,
          3.0355e+00, -4.1693e+00, -2.5328e+00, -9.6242e+00, -5.4168e+00],
        [-7.5866e+00, -9.3408e+00, -2.1233e+00, -4.3705e+00, -2.1037e+00,
          1.8128e+00, -1.0693e+01,  8.4773e+00, -9.7981e+00, -4.9864e+00],
        [-1.6687e+01,  1.6793e+00, -1.4550e+01, -1.4599e+01, -1.2505e+01,
         -9.9047e+00, -1.3652e+01, -1.1505e+01, -6.2283e+00,  9.7683e+00],
        [-4.0115e+00, -4.6556e+00, -3.8840e+00, -6.0950e+00, -1.4993e+00,
         -1.0334e+01, -2.5027e+00, -1.1730e+01,  7.4326e+00, -5.6403e+00],
        [-1.0580e+01, -6.9429e+00, -2.2634e+00,  1.5171e+00, -5.9256e+00,
          5.9894e+00, -4.5485e+00,  1.3362e+00, -1.3608e+01, -3.8857e+00],
        [-7.2214e+00, -9.6210e+00, -3.0361e+00, -4.8603e-01,  2.0060e+00,
          4.4839e-01, -1.5575e+00,  7.3342e+00, -6.5340e+00, -6.2954e+00],
        [-4.3000e+00, -3.7282e+00, -4.2558e+00, -7.2366e+00, -6.7670e+00,
         -7.0296e+00, -7.1621e+00, -1.2109e+01,  8.4541e+00, -2.1809e+00],
        [-1.0956e+01, -3.8419e+00, -4.3980e+00, -4.5993e+00, -3.7827e+00,
         -6.6216e+00,  8.5256e+00, -9.7110e+00, -5.9239e+00, -6.1639e+00],
        [-4.3121e+00, -6.1329e+00, -2.8060e+00, -1.6354e+00,  1.6602e-02,
         -2.2713e+00, -1.1629e+01,  7.5701e+00, -4.2084e+00, -4.5682e+00],
        [ 8.0907e+00, -6.0634e+00,  4.6659e+00, -1.6586e+00, -9.0147e+00,
         -8.1195e+00, -6.0529e+00, -8.9741e+00, -7.8234e+00, -8.5851e+00],
        [ 1.9034e+00, -9.7562e+00, -2.0267e+00, -3.7257e+00,  9.9355e+00,
         -6.2924e+00, -3.8345e+00, -6.3790e+00, -5.7387e+00, -1.1103e+01],
        [-3.0921e+01, -2.1621e+01, -5.4353e+01, -4.7245e+01, -3.7000e+01,
         -3.4863e+01, -4.0291e+01, -3.3883e+01, -2.9601e+01, -5.6254e+00],
        [-1.2905e+01, -1.3472e+01,  9.2327e-01, -2.8357e+00,  8.1883e+00,
          4.3325e+00, -4.3434e+00, -3.2120e+00, -1.2262e+01, -9.3329e+00],
        [-6.3847e+00, -1.1619e+01,  5.0406e+00,  2.1294e-01, -1.0996e+00,
         -1.6410e+00, -4.3977e-01, -2.5613e+00, -7.5707e+00, -5.7197e+00],
        [-9.4177e+00, -8.9703e+00, -1.4783e+00, -1.7230e+00,  6.4105e+00,
         -2.5606e+00, -1.4806e+00,  1.7309e+00, -1.1134e+01, -4.2722e+00],
        [ 4.6209e+00, -5.0047e+00,  3.0933e+00, -2.4168e+00, -4.0803e+00,
         -6.0362e+00, -2.9792e+00, -4.9186e+00, -7.6530e+00, -5.2772e+00],
        [-3.9561e+00,  1.2284e+00, -4.8849e+00, -6.9553e+00, -5.2604e+00,
         -5.6750e+00, -6.5588e+00, -3.1367e+00, -6.2173e+00,  6.3312e+00],
        [-1.0859e+01, -6.0620e+00,  1.3585e+00, -4.0465e+00, -2.8464e+00,
         -3.2949e+00,  9.2765e+00, -9.4153e+00, -1.0764e+01, -7.0800e+00],
        [-5.2258e+00, -3.4930e+00, -2.2008e+00,  1.8739e+00, -2.0247e+00,
         -1.4186e+00,  5.4284e+00, -5.3386e+00, -9.2889e+00, -5.8668e+00],
        [-9.7352e+00, -4.5637e+00, -3.2832e+00,  4.1532e+00, -2.4300e+00,
          6.1023e+00, -1.5339e+00, -6.2699e+00, -1.0843e+01, -5.9535e+00],
        [-5.3368e+00, -1.1030e+01, -5.2894e-01,  1.1336e+00,  4.8150e+00,
         -4.4578e-02, -3.7082e+00, -3.3569e+00, -4.1083e+00, -7.4827e+00],
        [-1.3147e+01, -2.6122e+00, -4.7112e-01, -2.2107e+00,  3.1113e-01,
          3.7294e+00, -1.5433e+00, -4.5551e+00, -8.1783e+00, -5.0212e-01],
        [-1.2552e+02, -1.2700e+02, -1.5882e+02, -1.6982e+02, -1.5183e+02,
         -1.3917e+02, -1.5181e+02, -1.0076e+02, -1.4325e+02, -8.1671e+01],
        [-5.7300e+00,  3.3258e+00, -6.8755e+00, -2.4311e+00, -1.0303e+01,
         -6.7725e+00, -5.1806e+00, -8.1225e+00,  6.0065e+00, -2.2572e+00],
        [-6.9506e+00, -1.0875e+01, -3.0312e+00, -4.1034e+00,  3.7097e+00,
          3.0983e+00, -8.0089e+00,  7.5264e+00, -9.4120e+00, -6.8525e+00],
        [ 1.7554e+00,  3.4273e+00, -1.0988e+01, -9.9291e+00, -1.3660e+01,
         -8.1717e+00, -1.0350e+01, -6.4892e+00, -1.8713e+00,  5.2901e+00],
        [-1.4639e+01, -9.2829e+00, -3.4846e+01, -2.9877e+01, -1.9012e+01,
         -1.9927e+01, -2.1857e+01, -1.8669e+01, -1.4889e+01,  1.9043e+00],
        [-9.1077e+00, -3.2261e+00, -5.7321e+00,  7.9698e-01, -5.0112e+00,
          9.7333e+00, -4.4414e+00, -5.6727e+00, -3.6533e+00, -5.2224e+00],
        [-1.8026e+00, -8.1759e+00, -2.6330e+00, -6.0893e+00,  9.0954e+00,
         -5.2027e+00, -1.0602e+01,  1.0168e+00, -8.4533e+00, -4.7865e+00],
        [-1.4655e+01, -6.4934e+00, -1.2591e+01, -5.2759e+00, -1.4172e+01,
         -1.1954e+01,  1.2004e+01, -1.7356e+01, -1.2593e+01, -1.4028e+01],
        [-3.8072e+00, -8.8345e+00, -1.9198e+00,  4.8915e+00, -1.0058e+01,
          1.7109e+00, -1.0878e+01,  6.4270e-01, -9.8807e+00, -2.5056e+00],
        [-1.3573e+01, -1.7093e+00, -6.2331e+00, -5.8733e+00, -4.5266e+00,
         -5.7839e+00,  8.3029e+00, -1.0978e+01, -8.4180e+00, -4.1966e+00],
        [ 5.6491e+00,  3.2079e-01, -4.1283e+00, -6.7249e+00, -6.6357e+00,
         -7.0581e+00, -5.9731e+00, -4.9353e+00, -1.9639e+00, -8.0638e-01],
        [-4.4991e+01, -4.6709e+01, -7.4977e+01, -7.1004e+01, -6.0830e+01,
         -5.8357e+01, -6.6140e+01, -4.7114e+01, -5.3086e+01, -2.1728e+01],
        [-9.5045e+00, -8.3238e+00,  1.6578e+00,  2.7818e+00, -3.1093e+00,
          2.8260e+00, -3.4302e-01, -4.3407e-01, -1.4178e+01, -5.3719e+00],
        [-2.8966e+00, -4.4248e+00, -6.4840e+00,  8.5072e-01, -8.6528e+00,
         -2.9140e+00, -5.4879e+00, -6.2466e+00,  3.0582e+00,  2.9907e-01],
        [-1.1027e+01, -1.4637e+01, -6.3754e+00, -7.9668e+00,  2.4818e+00,
          3.3689e+00, -1.1081e+01,  9.9560e+00, -1.6052e+01, -8.8443e+00],
        [-7.1463e+00, -7.1973e+00,  4.9795e+00, -4.9618e+00,  6.7284e-01,
         -1.7895e+00,  4.7026e+00, -6.8544e+00, -8.1530e+00, -6.9511e+00],
        [-6.9727e+00, -3.7220e+00, -2.8542e+00, -1.3481e+01, -6.2924e+00,
         -1.0089e+01, -4.5918e+00, -8.2975e+00, -4.9305e+00,  6.9602e+00],
        [-8.4215e+00, -4.2065e+00, -4.4165e+00, -8.5781e+00, -4.3038e+00,
         -9.4818e+00, -4.4825e+00, -1.0164e+01,  7.3309e+00, -1.4947e+00],
        [-1.4094e+00, -4.7571e+00,  7.3529e-01, -2.2995e+00, -8.9085e+00,
         -6.2306e-01, -6.0241e+00,  1.5970e-01, -9.3775e+00,  3.0020e-01],
        [-7.9991e+00, -7.1577e+00, -6.6275e-01,  3.4172e+00, -1.2110e+00,
          2.8573e+00,  3.1793e-03, -3.2014e+00, -1.0439e+01, -5.9814e+00],
        [-1.5954e+00, -4.1342e+00, -6.2907e+00, -8.5087e+00, -1.1240e+01,
         -9.7933e+00, -1.0777e+01, -1.5813e+01,  1.1484e+01, -3.8669e+00],
        [-9.7094e-01, -7.3009e+00, -4.3973e+00, -4.4078e+00, -5.1811e+00,
         -8.2652e+00, -7.9738e+00, -1.0542e+01,  9.4031e+00, -5.4497e+00],
        [-1.9248e+00, -1.1755e+01, -6.4050e+00, -2.0835e+00, -3.1244e+00,
         -1.6636e-01, -1.1126e+01,  7.5820e+00, -1.1455e+01, -3.4799e+00],
        [-8.1057e+00, -1.3345e+00, -4.8492e+00, -3.4641e-01, -5.2417e+00,
          3.3691e+00, -1.6310e+00, -7.0148e-01, -4.4962e+00, -1.0723e+00],
        [-6.3135e+00, -1.1394e+01, -1.4465e+00,  1.6135e+00, -1.4085e+00,
          2.9802e+00, -1.6158e+00,  1.3331e+00, -2.6474e+00, -6.9406e+00],
        [-4.1085e+00, -6.5062e+00, -4.4037e+00,  2.5355e+00,  8.6023e-01,
          2.3003e+00, -2.5135e+00,  2.2935e+00, -7.0287e+00, -6.9433e+00],
        [-8.6394e+00, -1.4250e+01, -8.0727e+00, -8.9137e+00,  1.7106e+00,
         -4.4408e+00, -9.6702e+00,  1.1776e+01, -1.4702e+01, -7.4480e+00],
        [-1.1296e+01, -7.0704e+00, -3.5470e+00,  3.1931e+00, -5.6829e+00,
          5.3717e+00, -4.0039e+00,  9.3444e-01, -1.4770e+01, -3.6484e+00],
        [-9.1493e+00, -1.9447e+00, -4.4174e+00, -4.2687e+00, -5.4117e+00,
         -3.4017e+00,  6.5238e+00, -7.8898e+00, -5.8720e+00, -2.2315e+00],
        [-5.2169e+00, -3.1946e+00, -5.9566e+00,  5.3343e+00, -6.8279e+00,
          4.4322e+00, -8.1105e+00, -5.8907e+00, -3.0082e+00, -1.2604e+00]],
       device='cuda:0', grad_fn=<AddmmBackward0>)
targets
tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,
        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,
        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3], device='cuda:0')
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
outputs
torch.Size([64, 10])
targets
torch.Size([64])
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
outputs
torch.Size([64, 10])
targets
torch.Size([64])
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
161
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Pruned 11764261 weights out of 23528522 weights total
Retraining CIFAR-10 after pruning...
Using device cuda
Files already downloaded and verified
Loading existing best weights from cifar10_best_model.pth
Pruning CIFAR-10 with 50.0% using method sensitivity...
Pruned 11764261 weights out of 23528522 weights total
Retraining CIFAR-10 after pruning...
Epoch: 1, Loss: 0.0422, Training Accuracy: 98.71%, Validation Accuracy: 77.04%
New best model found and saved with validation accuracy: 77.04%
Epoch: 2, Loss: 0.0422, Training Accuracy: 98.65%, Validation Accuracy: 77.27%
New best model found and saved with validation accuracy: 77.27%
Epoch: 3, Loss: 0.0415, Training Accuracy: 98.69%, Validation Accuracy: 77.17%
Epoch: 4, Loss: 0.0403, Training Accuracy: 98.73%, Validation Accuracy: 77.10%
Epoch: 5, Loss: 0.0402, Training Accuracy: 98.71%, Validation Accuracy: 76.94%
Epoch: 6, Loss: 0.0398, Training Accuracy: 98.70%, Validation Accuracy: 77.18%
Epoch: 7, Loss: 0.0408, Training Accuracy: 98.74%, Validation Accuracy: 77.32%
New best model found and saved with validation accuracy: 77.32%
Epoch: 8, Loss: 0.0390, Training Accuracy: 98.75%, Validation Accuracy: 77.08%
Epoch: 9, Loss: 0.0405, Training Accuracy: 98.70%, Validation Accuracy: 77.12%
Epoch: 10, Loss: 0.0387, Training Accuracy: 98.77%, Validation Accuracy: 77.04%
